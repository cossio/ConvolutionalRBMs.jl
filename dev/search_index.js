var documenterSearchIndex = {"docs":
[{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"EditURL = \"https://github.com/cossio/ConvolutionalRBMs.jl/blob/master/docs/src/literate/MNIST.jl\"","category":"page"},{"location":"literate/MNIST/#MNIST","page":"MNIST","title":"MNIST","text":"","category":"section"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"We begin by importing the required packages. We load MNIST via the MLDatasets.jl package.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"using CairoMakie, Statistics\nimport MLDatasets\nimport Flux\nimport RestrictedBoltzmannMachines as RBMs\nimport ConvolutionalRBMs\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Useful function to plot MNIST digits.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"\"\"\"\n    imggrid(A)\n\nGiven a four dimensional tensor `A` of size `(width, height, ncols, nrows)`\ncontaining `width x height` images in a grid of `nrows x ncols`, this returns\na matrix of size `(width * ncols, height * nrows)`, that can be plotted in a heatmap\nto display all images.\n\"\"\"\nfunction imggrid(A::AbstractArray{<:Any,4})\n    return reshape(permutedims(A, (1,3,2,4)), size(A,1)*size(A,3), size(A,2)*size(A,4))\nend","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Let's visualize some random digits.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\ndigits = MLDatasets.MNIST.traintensor()\ndigits = digits[:,:,rand(1:size(digits,3), nrows * ncols)]\ndigits = reshape(digits, 28, 28, ncols, nrows)\nimage!(ax, imggrid(digits), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Now load the full dataset.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"train_x, train_y = MLDatasets.MNIST.traindata()\ntests_x, tests_y = MLDatasets.MNIST.testdata()\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"train_x, tests_x contain the digit images, while train_y, tests_y contain the labels. We will train an RBM with binary (0,1) visible and hidden units. Therefore we binarize the data first. In addition, we restrict our attention to 0,1 digits only, so that training and so on are faster.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"selected_digits = (0, 1)\ntrain_x = train_x[:, :, train_y .∈ Ref(selected_digits)] .≥ 0.5\ntests_x = tests_x[:, :, tests_y .∈ Ref(selected_digits)] .≥ 0.5\ntrain_y = train_y[train_y .∈ Ref(selected_digits)]\ntests_y = tests_y[tests_y .∈ Ref(selected_digits)]\ntrain_nsamples = length(train_y)\ntests_nsamples = length(tests_y)\n(train_nsamples, tests_nsamples)","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"The above train_x and tests_x are BitArrays. Though convenient in terms of memory space, these are very slow in linear algebra. Since we frequently multiply data configurations times the weights of our RBM, we want to speed this up. So we convert to floats, which have much faster matrix multiplies thanks to BLAS. We will use Float32 here. To hit BLAS, this must be consistent with the types we use in the parameters of the RBM below.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Float = Float32\ntrain_x = Array{Float}(train_x)\ntests_x = Array{Float}(tests_x)\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Plot some examples of the binarized data.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\ndigits = reshape(train_x[:, :, rand(1:size(train_x,3), nrows * ncols)], 28, 28, ncols, nrows)\nimage!(ax, imggrid(digits), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Initialize an RBM with 100 hidden units. It is recommended to initialize the weights as random normals with zero mean and variance = 1/(number of visible units). See Glorot & Bengio 2010.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Notice how we pass the Float type, to set the parameter type of the layers and weights in the RBM.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"rbm = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,200), randn(Float,28,28,200)/28)\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Initially, the RBM assigns a poor pseudolikelihood to the data.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, train_x) |> mean","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, tests_x) |> mean","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Incidentally, let us see how long it takes to evaluate the pseudolikelihood on the full dataset.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"@elapsed RBMs.log_pseudolikelihood(rbm, train_x) # pre-compiled by the calls above","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"This is the cost you pay when training by tracking the pseudolikelihood. The pseudolikelihood is computed on the full dataset every epoch. So if this time is too high compared to the computational time of training on an epoch, we should disable tracking the pseudolikelihood.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Now we train the RBM on the data. This returns a MVHistory object containing things like the pseudo-likelihood of the data during training. We print here the time spent in the training as a rough benchmark.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"@time history = RBMs.pcd!(\n    rbm, train_x; epochs=200, batchsize=256, optimizer=Flux.ADAM()\n)\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Plot of log-pseudolikelihood during learning. Note that this shows the pseudolikelihood of the train data.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"lines(get(history, :lpl)...)","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Notice the abrupt oscillations at the beginning. Those come from the poor initialization of the RBM. We will correct this below.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Now let's generate some random RBM samples. First, we select random data digits to be initial conditions for the Gibbs sampling, and let's plot them.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Now we do the Gibbs sampling to generate the RBM digits.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"@elapsed fantasy_x = RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Plot the resulting samples.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"fig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/#Parameter-initialization","page":"MNIST","title":"Parameter initialization","text":"","category":"section"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"If we initialize parameters, in particular matching the single-site statistics, the model trains better and faster.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"rbm = RBMs.RBM(\n    RBMs.Binary(Float,28,28),\n    RBMs.Binary(Float,200),\n    randn(Float,28,28,200)/28\n)\nRBMs.initialize!(rbm, train_x) # match single-site statistics\n@time history_init = RBMs.pcd!(\n    rbm, train_x; epochs=200, batchsize=256, optimizer=Flux.ADAM()\n)\nnothing #hide","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Compare the learning curves, with and without initialization.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"fig = Figure(resolution=(800, 300))\nax = Axis(fig[1,1])\nlines!(ax, get(history, :lpl)..., label=\"no init.\")\nlines!(ax, get(history_init, :lpl)..., label=\"init.\")\naxislegend(ax, position=:rb)\nfig","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Notice how the pseudolikelihood curve grows a bit faster than before and is smoother. The initial oscillations are gone.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, train_x) |> mean","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, tests_x) |> mean","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"Let's look at some samples generated by this RBM.","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"fantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]\nfantasy_x .= RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"","category":"page"},{"location":"literate/MNIST/","page":"MNIST","title":"MNIST","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [ConvolutionalRBMs]","category":"page"},{"location":"reference/#ConvolutionalRBMs.ConvBinary","page":"Reference","title":"ConvolutionalRBMs.ConvBinary","text":"ConvBinary(θ)\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConvolutionalRBMs._visconv","page":"Reference","title":"ConvolutionalRBMs._visconv","text":"_visconv(w, v)\n\nInternal function used to compute inputs from a visible configuration v to the hidden layer, where w are the RBM weights.\n\nI_mu^muk_1dotsk_nb = sum_i_1dotsi_n w_i_1dotsi_nmu v_i_1+k_1-1dotsi_n+k_n-1b\n\nAssumes that:\n\nw is of size (N₁, N₂, ..., Nₙ, M)\nv is of size (V₁, V₂, ..., Vₙ, B)\n\nwhere M is the number of hidden units and B is the batch size. Therefore the hidden and batch dimension must be flattened before calling _visconv. The output I is of size (V₁ - N₁ + 1, V₁ - N₂ + 1, ..., Vₙ - Nₙ + 1, B).\n\nwarning: Warning\nThis is an internal function and is not part of the public API. It can change in future (minor non-breaking) versions.\n\nwarning: Warning\nOnly works for n = 1, 2, 3, 4. Due to a technical limitation of Tullio.jl (#129), _visconv is defined by hand for different values of n. Therefore only a small number can be supported. If you need some higher value of n, consider doing a PR to define the corresponding _visconv method.\n\n\n\n\n\n","category":"function"},{"location":"#ConvolutionalRBMs.jl-Documentation","page":"Home","title":"ConvolutionalRBMs.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package to train and simulate convolutional Restricted Boltzmann Machines. The package is not registered. Install with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/cossio/ConvolutionalRBMs.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package doesn't export any symbols.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Most of the functions have a helpful docstring. See Reference section.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See also the Examples listed on the menu on the left side bar.","category":"page"},{"location":"#Related","page":"Home","title":"Related","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See also https://github.com/cossio/RestrictedBoltzmannMachines.jl.","category":"page"}]
}
